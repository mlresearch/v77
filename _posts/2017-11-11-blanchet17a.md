---
title: Distributionally Robust Groupwise Regularization Estimator
abstract: Regularized estimators in the context of group variables have been applied
  successfully in model and feature selection in order to preserve interpretability.
  We formulate a Distributionally Robust Optimization (DRO) problem which recovers
  popular estimators, such as Group Square Root Lasso (GSRL). Our DRO formulation
  allows us to interpret GSRL as a game, in which we learn a regression parameter
  while an adversary chooses a perturbation of the data. We wish to pick the parameter
  to minimize the expected loss under any plausible model chosen by the adversary
  - who, on the other hand, wishes to increase the expected loss. The regularization
  parameter turns out to be precisely determined by the amount of perturbation on
  the training data allowed by the adversary. In this paper, we introduce a data-driven
  (statistical) criterion for the optimal choice of regularization, which we evaluate
  asymptotically, in closed form, as the size of the training set increases. Our easy-to-evaluate
  regularization formula is compared against cross-validation, showing comparable
  performance.
section: Accepted Papers
layout: inproceedings
series: Proceedings of Machine Learning Research
id: blanchet17a
month: 0
tex_title: Distributionally Robust Groupwise Regularization Estimator
firstpage: 97
lastpage: 112
page: 97-112
order: 97
cycles: false
author:
- given: Jose
  family: Blanchet
- given: Yang
  family: Kang
date: 2017-11-11
address: 
publisher: PMLR
container-title: Proceedings of the Ninth Asian Conference on Machine Learning
volume: '77'
genre: inproceedings
issued:
  date-parts:
  - 2017
  - 11
  - 11
pdf: http://proceedings.mlr.press/v77/blanchet17a/blanchet17a.pdf
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v77/blanchet17a/blanchet17a-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
